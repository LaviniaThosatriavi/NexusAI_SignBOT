{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f400c504",
   "metadata": {},
   "source": [
    "\n",
    "## Text/Speech to Sign Language Translator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202cad7d",
   "metadata": {},
   "source": [
    "## 0. Importing and installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65268a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (3.10.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from SpeechRecognition) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n",
      "Requirement already satisfied: nltk in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\marcell suyanto\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install SpeechRecognition\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c457601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Marcell\n",
      "[nltk_data]     Suyanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Marcell\n",
      "[nltk_data]     Suyanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Marcell\n",
      "[nltk_data]     Suyanto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing and downlaoding all the libraries and functions\n",
    "import nltk\n",
    "import cv2\n",
    "import time\n",
    "import speech_recognition as sr\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "wnl = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfb290",
   "metadata": {},
   "source": [
    "## 1. Taking speech from user & Converting it into text\n",
    "Library used : speech_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "493060c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speech to text\n",
    "r = sr.Recognizer()\n",
    "mic = sr.Microphone()\n",
    "\n",
    "def speech2text():\n",
    "    global text\n",
    "    with mic as audio_file:\n",
    "        print(\"Speak Now...\")\n",
    "        \n",
    "        r.adjust_for_ambient_noise(audio_file)\n",
    "        audio = r.listen(audio_file)\n",
    "        \n",
    "        print(\"Converting Speech to Text...\")\n",
    "        text= r.recognize_google(audio)\n",
    "        text=text.lower()\n",
    "       \n",
    "        print(\"Input:\",text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4771cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak Now...\n",
      "Converting Speech to Text...\n",
      "Input: my name is joe\n"
     ]
    }
   ],
   "source": [
    "def SorT():\n",
    "    speech_or_text=str(input(\"Type S, if you want to enter speech as input \\n else type T if you want to enter text as input.\\n\"))\n",
    "    if speech_or_text.lower() == \"s\":\n",
    "        text = speech2text()\n",
    "    elif speech_or_text.lower() == \"t\":\n",
    "        text = str(input(\"Enter the text:\"))\n",
    "    else:\n",
    "        print(\"Invalid input. Either enter T or S.\")\n",
    "        SorT()\n",
    "    return text\n",
    "text = SorT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f22f3a",
   "metadata": {},
   "source": [
    "## 2. Processing the text using bag of words algorithm\n",
    "Library used: nltk\n",
    "<ul>\n",
    "    <li>Removing the stopwords, including punctuations</li>\n",
    "    <li>Convert text to lower case</li>\n",
    "    <li>Tokenize the sentence</li>\n",
    "    <li>Lemmatize the tokens </li>\n",
    "    <li>Finally, creating the list of the processed tokens</li>\n",
    " </ul>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd458d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of stop words or useless words \n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "stop_words=['@','#',\"http\",\":\",\"is\",\"the\",\"are\",\"am\",\"a\",\"it\",\"was\",\"were\",\"an\",\",\",\".\",\"?\",\"!\",\";\",\"/\"]\n",
    "for i in stop_words:\n",
    "    stop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd4b8d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['name', 'joe']\n"
     ]
    }
   ],
   "source": [
    "#processing the text using bag of wor\n",
    "tokenized_text = nltk.tokenize.word_tokenize(text)\n",
    "lemmed = [wnl.lemmatize(word) for word in tokenized_text]\n",
    "processed=[]\n",
    "for i in lemmed :\n",
    "    if i.lower() == \"i\":\n",
    "        processed.append(\"me\")\n",
    "    elif i not in stop:\n",
    "        i=i.lower()\n",
    "        processed.append((i))\n",
    "print(\"Keywords:\",processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d31e2",
   "metadata": {},
   "source": [
    "## 3. Showing animation of the keywords\n",
    "Library used: cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d5bb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing animation of the keywords.\n",
    "assets_list=['0.mp4', '1.mp4', '2.mp4', '3.mp4', '4.mp4', '5.mp4','6.mp4', '7.mp4', '8.mp4', '9.mp4', 'a.mp4', 'after.mp4',\n",
    "             'again.mp4', 'against.mp4', 'age.mp4', 'all.mp4', 'alone.mp4','also.mp4', 'and.mp4', 'ask.mp4', 'at.mp4', 'b.mp4', 'be.mp4',\n",
    "             'beautiful.mp4', 'before.mp4', 'best.mp4', 'better.mp4', 'busy.mp4', 'but.mp4', 'bye.mp4', 'c.mp4', 'can.mp4', 'cannot.mp4',\n",
    "             'change.mp4', 'college.mp4', 'come.mp4', 'computer.mp4', 'd.mp4', 'day.mp4', 'distance.mp4', 'do not.mp4', 'do.mp4', 'does not.mp4',\n",
    "             'e.mp4', 'eat.mp4', 'engineer.mp4', 'f.mp4', 'fight.mp4', 'finish.mp4', 'from.mp4', 'g.mp4', 'glitter.mp4', 'go.mp4', 'god.mp4',\n",
    "             'gold.mp4', 'good.mp4', 'great.mp4', 'h.mp4', 'hand.mp4', 'hands.mp4', 'happy.mp4', 'hello.mp4', 'help.mp4', 'her.mp4', 'here.mp4',\n",
    "             'his.mp4', 'home.mp4', 'homepage.mp4', 'how.mp4', 'i.mp4', 'invent.mp4', 'it.mp4', 'j.mp4', 'k.mp4', 'keep.mp4', 'l.mp4', 'language.mp4', 'laugh.mp4',\n",
    "             'learn.mp4', 'm.mp4', 'me.mp4', 'mic3.png', 'more.mp4', 'my.mp4', 'n.mp4', 'name.mp4', 'next.mp4', 'not.mp4', 'now.mp4', 'o.mp4', 'of.mp4', 'on.mp4',\n",
    "             'our.mp4', 'out.mp4', 'p.mp4', 'pretty.mp4', 'q.mp4', 'r.mp4', 'right.mp4', 's.mp4', 'sad.mp4', 'safe.mp4', 'see.mp4', 'self.mp4', 'sign.mp4', 'sing.mp4', \n",
    "             'so.mp4', 'sound.mp4', 'stay.mp4', 'study.mp4', 't.mp4', 'talk.mp4', 'television.mp4', 'thank you.mp4', 'thank.mp4', 'that.mp4', 'they.mp4', 'this.mp4', 'those.mp4', \n",
    "             'time.mp4', 'to.mp4', 'type.mp4', 'u.mp4', 'us.mp4', 'v.mp4', 'w.mp4', 'walk.mp4', 'wash.mp4', 'way.mp4', 'we.mp4', 'welcome.mp4', 'what.mp4', 'when.mp4', 'where.mp4', \n",
    "             'which.mp4', 'who.mp4', 'whole.mp4', 'whose.mp4', 'why.mp4', 'will.mp4', 'with.mp4', 'without.mp4', 'words.mp4', 'work.mp4', 'world.mp4', 'wrong.mp4', 'x.mp4', 'y.mp4',\n",
    "             'you.mp4', 'your.mp4', 'yourself.mp4', 'z.mp4']\n",
    "tokens_sign_lan=[]\n",
    "for word in processed:\n",
    "    string = str(word+\".mp4\")\n",
    "    if string in assets_list:\n",
    "        tokens_sign_lan.append(str(\"assets/\"+string))\n",
    "    else:\n",
    "        for j in word:\n",
    "            tokens_sign_lan.append(str(\"assets/\"+j+\".mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "129bcb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing animation\n",
    "\n",
    "for i in tokens_sign_lan:\n",
    "    label = i.replace(\"assets/\",\"\").replace(\".mp4\",\"\")\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    fps= int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error File Not Found\")\n",
    "    while cap.isOpened():\n",
    "        ret,frame= cap.read()\n",
    "        if ret:\n",
    "            time.sleep(1/fps)\n",
    "            cv2.putText(frame, label,(10,60),cv2.FONT_HERSHEY_SIMPLEX, 1,(0,255,0), 1, cv2.LINE_AA)\n",
    "            cv2.imshow('frame', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
